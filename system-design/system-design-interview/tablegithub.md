| |  | |
|:----|:----|:----|
| |  | |
| | | System Design / Application                  | Service                           | Database            | Database Type                      | Why Chosen                                                                                              | Data Consistency Needs | Scale Requirements | Communication Protocol | Why Chosen for Protocol                                                                                              | Additional Details                                                                                                                           | Key Insights for System Design Interviews                                                                                           | Specific Knowledge to Learn for Interviews                                                                                                    | | |
| | |----------------------------------------------|-----------------------------------|---------------------|------------------------------------|---------------------------------------------------------------------------------------------------------|------------------------|--------------------|------------------------|----------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------| | |
| | | **API Rate Limiter**                        | Tracking Request Counts           | Redis               | NoSQL (In-memory Key-Value Store)  | Ultra-low latency for request counting, atomic increments (INCR command) for accuracy, built-in expiration (TTL) for auto-reset, simple implementation. | Strong                 | High               | Redis Pub/Sub          | Redis Pub/Sub enables real-time broadcasting of rate limit updates across distributed instances, ensuring consistent enforcement in a scalable, low-latency manner critical for API throttling. | Data Model: Key-Value. Key: rate_limit:{user_id}:{endpoint}, Value: request_count (integer). Uses Redis INCR for atomicity and EXPIRE for TTL. Multiple API servers subscribe to rate limit channels to synchronize counts instantly, preventing overages. | Logic: Implement token bucket or leaky bucket algorithms for rate limiting. Understand distributed rate limiting across servers. Data Structure: Use Redis Sorted Sets for sliding window rate limiting. Algorithm: Atomic operations (INCR, EXPIRE) ensure accuracy in high-concurrency scenarios. Interview Tip: Discuss trade-offs between fixed-window and sliding-window rate limiting, and Redis’ role in distributed consistency. | Learn how API gateways (e.g., Kong, AWS API Gateway) implement rate limiting. Understand handling burst traffic with token bucket nuances (e.g., bucket size, refill rate). Explain Redis’ memory efficiency for millions of users and sharding keys across Redis clusters for massive scale. Highlight real-world examples like GitHub’s API throttling. | | |
| | |                                              | Persistent Rate Limit Rules       | PostgreSQL          | RDBMS                             | Strong consistency and ACID compliance for reliable storage of rate limit configurations; rules are critical and must be durably persisted. | Strong                 | Medium             | REST over HTTP         | REST provides a stateless, standardized interface using HTTP methods (GET, POST, PUT) to manage rules, ensuring easy integration with admin dashboards and API services. | Schema: rate_limit_rules table with columns: rule_id (SERIAL PRIMARY KEY), user_id (INTEGER, INDEX), endpoint (VARCHAR), limit (INTEGER), time_window_seconds (INTEGER), created_at (TIMESTAMP). REST endpoints (e.g., /rules/{rule_id}) allow CRUD operations, with PostgreSQL ensuring transactional integrity for rule updates. | Logic: Rules must be versioned and auditable; consider caching rules in memory for faster access. Data Structure: Use indexed columns for quick rule lookups. Algorithm: Implement rule precedence and conflict resolution logic. Interview Tip: Explain handling rule updates without downtime (e.g., blue-green deployments, feature flags). | Understand designing a schema for hierarchical rules (e.g., per-user vs. per-endpoint). Know PostgreSQL’s EXPLAIN to optimize queries for rule retrieval. Discuss rule propagation latency in distributed systems and syncing rules with Redis for runtime enforcement (e.g., via a background job). Real-world case: Twitter’s rate limit rule management. | | |
| | | **Key-Value Store**                         | Core Key-Value Storage            | DynamoDB            | NoSQL (Distributed Key-Value Store) | High scalability (auto-scaling), low latency, managed service (serverless reduces overhead), tunable consistency for flexibility. | Tunable (Eventual/Strong) | Massive            | REST over HTTP         | REST leverages HTTP’s scalability and statelessness for key-value operations (GET, PUT, DELETE), integrating seamlessly with client apps and microservices via standard APIs. | Data Model: key_value_store table with key (String, Primary Key) and value (Binary/String). DynamoDB’s REST API (e.g., GET /items/{key}) supports massive-scale reads/writes. Tunable consistency balances latency vs. accuracy in distributed environments like web apps or caches. | Logic: Understand DynamoDB’s partitioning and partition key design for even distribution. Data Structure: Use composite keys for efficient querying. Algorithm: Implement consistent hashing for data distribution. Interview Tip: Discuss handling hot partitions and eventual consistency’s impact on read-after-write. | Master DynamoDB’s capacity modes (on-demand vs. provisioned) and calculate RCUs/WCUs for scale. Design keys to avoid hot partitions (e.g., adding randomness to keys). Explain DynamoDB Streams for change data capture and real-world use cases like AWS’s own services or Netflix’s caching layer. | | |
| | |                                              | Caching Frequently Accessed Keys  | Redis               | NoSQL (In-memory Key-Value Store)  | Sub-millisecond latency for hot data, in-memory speed, expiration (TTL) policies (LRU, LFU) for freshness, optimizes read performance. | Eventual               | High               | Redis Pub/Sub          | Redis Pub/Sub notifies distributed systems of cache updates/invalidations in real time, ensuring consistency across nodes with minimal latency, ideal for high-read scenarios. | Data Model: Key-Value with keys as identifiers and values as serialized objects (e.g., JSON). Supports complex structures (Hashes, Lists). Services subscribe to channels (e.g., cache_updates) to invalidate or refresh caches instantly, reducing backend load in high-traffic apps like e-commerce or gaming. | Logic: Implement cache eviction policies (LRU, LFU) and invalidation strategies. Data Structure: Use Redis Hashes for structured data caching. Algorithm: Consider cache-aside vs. read-through patterns. Interview Tip: Explain handling cache stampedes and using Bloom filters for optimization. | Know Redis’ eviction policies (e.g., maxmemory-policy) and tune them for workloads. Understand Redis Cluster’s slot-based sharding for scale. Discuss cache stampede solutions (e.g., probabilistic early expiration) and real-world examples like Twitter’s use of Redis for timeline caching. | | |
| | | **Unique ID Generator**                     | ID Generation                     | Twitter Snowflake   | Specialized ID Generation System  | Generates globally unique, k-sortable IDs with low latency and high throughput in distributed systems, avoids RDBMS sequence bottlenecks. | Strong                 | High               | Kafka                  | Kafka ensures ordered, high-throughput distribution of IDs across distributed systems, maintaining uniqueness and sequence with fault tolerance, critical for scalable ID generation. | Data Model: 64-bit integer (Timestamp: 41 bits, Machine/Datacenter ID: 10 bits, Sequence: 12 bits). Kafka topics (e.g., unique_ids) publish IDs to consumers (e.g., microservices), ensuring uniqueness in systems like social media or logging where millions of IDs are generated daily. | Logic: Understand Snowflake’s bit allocation and clock drift handling. Data Structure: Combine timestamp, node ID, and sequence number. Algorithm: Implement distributed ID generation avoiding collisions. Interview Tip: Discuss scaling across data centers and handling high concurrency without duplicates. | Learn Snowflake’s bit breakdown and adjust for longer epochs or more nodes. Understand clock synchronization challenges (e.g., NTP) and fallback strategies (e.g., incrementing sequence). Explain Twitter’s implementation and alternatives like UUIDv7 or MongoDB’s ObjectID for specific use cases. | | |
| | |                                              | ID Metadata Storage               | PostgreSQL          | RDBMS                             | Strong consistency and ACID compliance for reliable metadata storage (e.g., timestamp, node ID); metadata must be durable. | Strong                 | Medium             | REST over HTTP         | REST offers a simple, stateless API for querying and managing ID metadata, compatible with various clients and services, ensuring reliable access in distributed setups. | Schema: id_metadata table with generated_id (BIGINT PRIMARY KEY), generation_timestamp (TIMESTAMP), generator_node_id (INTEGER), metadata_value (JSONB). REST endpoints (e.g., /metadata/{id}) retrieve metadata, with PostgreSQL ensuring consistency for audit trails or debugging. | Logic: Optimize metadata storage for read-heavy workloads. Data Structure: Use indexes on frequently queried fields. Algorithm: Implement efficient querying for ID ranges or time-based searches. Interview Tip: Explain sharding metadata for scalability and backup strategies for durability. | Know PostgreSQL’s GIN indexes on JSONB for flexible metadata queries. Understand partitioning strategies (e.g., by time or node) for scale. Discuss audit requirements (e.g., GDPR compliance) and real-world use like Twitter’s ID metadata for tweet tracing. | | |
| | | **URL Shortening Service (Bitly, TinyURL)** | Short URL Lookup and Redirection  | Redis               | NoSQL (In-memory Key-Value Store)  | Ultra-low latency for redirection lookups (critical for UX), in-memory cache, TTL evicts less accessed URLs, cost-efficient. | Eventual               | High               | REST over HTTP         | REST provides a stateless, lightweight interface for URL redirection (e.g., GET /{short_code}), leveraging HTTP redirects (301/302) for fast, universal client compatibility. | Data Model: Key: short_url_code (e.g., "xyz123"), Value: original_url. Redis handles lookups with sub-millisecond latency. REST endpoint (e.g., GET /xyz123) returns a 301 redirect to the original URL, optimized for massive redirection traffic in shortening services. | Logic: Implement base62 encoding for short code generation. Data Structure: Use a hash map for quick lookups. Algorithm: Handle collisions in short code generation. Interview Tip: Discuss scaling for billions of URLs and analytics for click tracking. | Master base62 encoding implementation (e.g., converting integers to alphanumeric strings) and collision avoidance (e.g., pre-checking Redis). Learn Bitly’s approach to custom aliases and analytics integration (e.g., clicks per short URL). Explain HTTP redirect types (301 vs. 302) and their SEO impact. | | |
| | |                                              | Persistent URL Mapping Storage    | Cassandra           | NoSQL (Distributed Wide-Column Store) | High write scalability for billions of mappings, fault tolerance for durability, tunable consistency for read performance. | Tunable (Eventual/Strong) | Massive            | REST over HTTP         | REST enables scalable, stateless management of URL mappings (e.g., POST /urls to create), integrating easily with frontend apps and backend services for persistent storage ops. | Data Model: url_mappings table with short_url_prefix (Partition Key), short_url_code_suffix (Clustering Key), original_url (TEXT). REST APIs (e.g., POST /urls) store mappings, with Cassandra’s distributed nature handling massive write loads from global users creating short URLs daily. | Logic: Design partitioning to avoid hot partitions. Data Structure: Use wide rows for efficient storage. Algorithm: Implement distributed counters for analytics. Interview Tip: Explain handling URL expiration and Bloom filters for existence checks. | Understand Cassandra’s WITH COMPACT STORAGE for space efficiency in URL mappings. Learn TinyURL’s partitioning (e.g., by region or time) and TTL-based expiration. Discuss analytics integration (e.g., click counts) and real-world scale challenges like Bitly’s 20 billion URLs. | | |
| | | **Pastebin**                                | Snippet Storage and Retrieval     | MongoDB             | NoSQL (Document Store)            | Flexible schema for diverse snippet content (code, text, formatting), document-oriented model aligns with snippet structure. | Eventual               | Medium             | REST over HTTP         | REST offers a straightforward, stateless API for CRUD operations on snippets (e.g., POST /snippets), widely compatible with web/mobile clients for easy sharing and retrieval. | Data Model: snippets collection with documents: { _id: ObjectId, title: "My Code", content: "...", language: "python", created_at: ISODate() }. REST endpoints (e.g., GET /snippets/{id}) fetch snippets, with MongoDB’s flexibility supporting varied snippet types in sharing platforms. | Logic: Implement versioning for snippets to handle edits. Data Structure: Use embedded documents for comments or revisions. Algorithm: Implement text diffing for version control. Interview Tip: Discuss handling large snippets and search with text indexes. | Learn MongoDB’s sharding by _id or created_at for scale. Understand Pastebin’s expiration feature (e.g., TTL indexes) and securing snippets (e.g., private links). Explain version control (e.g., storing diffs) and real-world use cases like GitHub Gists’ snippet sharing. | | |
| | |                                              | Full-Text Search                  | Elasticsearch       | Search Engine                     | Fast full-text search over snippet content, inverted index for efficient keyword searching, enhances UX for quick discovery. | Eventual               | Medium             | REST over HTTP         | REST leverages HTTP’s simplicity and caching for querying the search index (e.g., GET /search?q=python), delivering fast, scalable search results to users. | Data Model: snippets_index with documents: { snippet_id: String, title: "My Code", content: "..." }. REST API (e.g., GET /search?q=python) queries Elasticsearch, which uses inverted indexes for rapid keyword lookups, critical for snippet discovery by users or developers. | Logic: Use analyzers for code-specific search (e.g., tokenizing code syntax). Data Structure: Inverted index with term frequency. Algorithm: Implement relevance scoring for search results. Interview Tip: Explain real-time indexing and trade-offs between latency and freshness. | Master Elasticsearch’s custom analyzers for code (e.g., whitespace vs. n-gram tokenizers). Learn Pastebin’s syntax-aware search (e.g., Python vs. Java keywords). Discuss index refresh intervals and scaling search for millions of snippets, citing real-world examples like Stack Overflow. | | |
| | |                                              | Caching Popular Snippets          | Redis               | NoSQL (In-memory Key-Value Store)  | Low-latency access to frequently accessed snippets, reduces MongoDB load, improves performance and responsiveness. | Eventual               | Medium             | Redis Pub/Sub          | Redis Pub/Sub notifies services of snippet updates or popularity shifts in real time, ensuring cache consistency across distributed systems with minimal latency. | Data Model: Key: snippet:{snippet_id}, Value: JSON-serialized snippet. Cache-Aside pattern with Redis. Services subscribe to snippet_updates channel for invalidation events (e.g., edits), ensuring fresh popular snippets are served quickly in high-traffic Pastebin-like systems. | Logic: Implement scoring for popularity (e.g., views, likes). Data Structure: Use Redis Sorted Sets to rank snippets by popularity. Algorithm: Use time-decaying popularity score. Interview Tip: Discuss cache invalidation and handling cache misses efficiently. | Understand Redis’ ZRANGE and ZINCRBY for dynamic popularity ranking. Learn Pastebin’s trending snippets approach (e.g., view counters) and preventing cache thrashing with TTL tweaks. Explain real-world caching patterns like Pastebin’s front-page optimization during viral snippet spikes. | | |
| | | **Chat System (WhatsApp, Slack, Snapchat)** | Persistent Message Storage        | MongoDB             | NoSQL (Document Store)            | Flexible schema for diverse message types (text, media, reactions), unstructured data handling, document storage for history. | Eventual               | High               | REST over HTTP         | REST provides a reliable, stateless API for storing/retrieving message history (e.g., POST /messages), ensuring compatibility with clients for asynchronous message access. | Data Model: messages collection with { _id: ObjectId, sender_id: ObjectId, chat_room_id: ObjectId, content: "..." }. REST endpoints (e.g., GET /chat/{room_id}/messages) fetch history, with MongoDB scaling for millions of chat messages in apps like WhatsApp. | Logic: Implement message retention policies and archival strategies. Data Structure: Use sharding by chat room ID for scalability. Algorithm: Implement message pagination for efficient retrieval. Interview Tip: Discuss delivery status (sent, delivered, read) and end-to-end encryption. | Learn WhatsApp’s sharding strategy (e.g., by user or room) and MongoDB’s TTL for message expiration (e.g., Snapchat’s ephemeral chats). Understand encryption key management (e.g., double ratchet algorithm) and storing metadata like read receipts. Discuss WhatsApp’s 100B messages/day scale. | | |
| | |                                              | Real-Time Messaging               | Redis               | NoSQL (In-memory Key-Value Store)  | Ultra-low latency for pub/sub messaging, fast broadcasting to participants, in-memory speed paramount for responsiveness. | Eventual               | High               | Redis Pub/Sub          | Redis Pub/Sub enables real-time, bi-directional message delivery with low latency, broadcasting messages instantly to all subscribers in a chat room, ideal for live chat. | Data Model: Channels: chat_room:{chat_room_id}. Messages published to channels reach subscribers (clients) instantly. No persistence in Redis Pub/Sub; chat apps use this for instant messaging, with clients subscribing to room channels for seamless, low-latency communication. | Logic: Implement queuing for offline users. Data Structure: Use Redis Streams for ordered message delivery. Algorithm: Handle message acknowledgments and retries. Interview Tip: Explain scaling Redis Pub/Sub and using WebSockets for clients. | Master Redis Streams’ consumer groups for reliable delivery (e.g., Slack’s channel messaging). Learn Slack’s use of WebSockets with Redis for real-time updates and handling offline message queues (e.g., fanout to Kafka). Discuss Snapchat’s ephemeral message lifecycle and WhatsApp’s HSM for business chats. | | |
| | |                                              | User Presence and Status          | Redis               | NoSQL (In-memory Key-Value Store)  | Fast reads/writes for frequent presence updates, in-memory storage ideal for transient, real-time data (online/offline, last seen). | Eventual               | High               | Redis Pub/Sub          | Redis Pub/Sub broadcasts presence changes (e.g., online/offline) in real time to all relevant clients, ensuring instant visibility of user status across the system. | Data Model: Key: presence:{user_id}, Value: { status: "online", last_seen: timestamp }. Clients subscribe to presence_updates channel for live status updates, critical for chat apps showing who’s online instantly without polling. | Logic: Implement heartbeat mechanisms to detect activity. Data Structure: Use Redis Sets for group presence (e.g., online users in a room). Algorithm: Use expiration timers for status updates. Interview Tip: Discuss handling large concurrent users and presence aggregation. | Learn Slack’s presence system (e.g., heartbeat intervals) and how Redis handles millions of users (e.g., WhatsApp’s 2B users). Understand SEXPIRE for automatic offline detection and aggregating presence for group chats. Explain WhatsApp’s “last seen” feature and privacy controls like status hiding. | | |
| | | **Distributed Email Service (Gmail, Outlook)** | Email Metadata Storage           | PostgreSQL          | RDBMS                             | Strong consistency for core metadata (sender, recipient, subject, timestamps), ACID compliance for reliable operations. | Strong                 | High               | REST over HTTP         | REST provides a scalable, stateless API for managing email metadata (e.g., GET /emails/{id}), ensuring compatibility with email clients and services for reliable access. | Schema: emails table with email_id (UUID PRIMARY KEY), sender_id (INTEGER), subject (VARCHAR), etc. REST endpoints (e.g., GET /user/{id}/emails) fetch metadata, with PostgreSQL ensuring consistency for millions of emails in systems like Gmail. | Logic: Implement threading and conversation views. Data Structure: Use indexes on sender, recipient, timestamp for efficient querying. Algorithm: Implement spam filtering and categorization. Interview Tip: Discuss handling attachments and metadata search. | Understand Gmail’s threading (e.g., conversation_id grouping) and PostgreSQL’s FULL TEXT SEARCH for subject lookups. Learn Outlook’s IMAP syncing and sharding emails by user or time. Explain Gmail’s label system vs. folders and its impact on schema design (e.g., many-to-many tags). | | |
| | |                                              | Email Content Storage             | S3/GCS              | Object Storage                    | Scalable, durable storage for email bodies and attachments, cost-effective for large, unstructured content. | Eventual               | Massive            | REST over HTTP         | REST enables efficient upload/retrieval of large email content (e.g., PUT /content/{email_id}), leveraging HTTP’s ability to handle binary data and integrate with object storage. | Data Model: Key: email_content:{email_id}, Object: MIME-format content. REST API (e.g., GET /content/{email_id}) retrieves email bodies from S3, linked to PostgreSQL metadata, supporting massive-scale storage for email attachments in distributed email services. | Logic: Implement deduplication and compression. Data Structure: Use hierarchical storage for attachments. Algorithm: Implement virus scanning and filtering. Interview Tip: Explain handling large attachments and pre-signed URLs for secure access. | Learn Gmail’s S3-like storage with MIME parsing and deduplicating attachments (e.g., SHA-256 hashing). Understand GCS’s lifecycle policies for archiving old emails and Outlook’s PST file integration. Discuss pre-signed URL security and CDN integration for faster attachment downloads. | | |
| | |                                              | Caching Recent Emails             | Redis               | NoSQL (In-memory Key-Value Store)  | Low-latency access to recent emails, reduces load on metadata storage during peak access, improves UX. | Eventual               | High               | Redis Pub/Sub          | Redis Pub/Sub notifies services of new email arrivals or updates, ensuring cache consistency across distributed email clients with real-time synchronization. | Data Model: Key: email_cache:{user_id}:{folder_id}, Value: List of JSON email metadata. Cache invalidation via email_updates channel on new emails, enhancing performance where users frequently access recent messages. | Logic: Implement pre-warming for frequently accessed folders. Data Structure: Use Redis Lists or Sorted Sets for ordered caching. Algorithm: Implement expiration based on activity. Interview Tip: Discuss cache consistency and invalidation strategies. | Understand Gmail’s “Priority Inbox” caching and Redis’ LRANGE for paginated email lists. Learn Outlook’s offline caching and IMAP sync. Explain cache invalidation on email deletion and real-world scale like Gmail’s 1.8B users accessing recent emails instantly. | | |
| | | **Social Media Platforms (Instagram, Facebook, Twitter, LinkedIn, Reddit)** | User Metadata Storage | PostgreSQL          | RDBMS                             | Strong consistency for profiles, relationships (followers, friends), ACID compliance for critical user data integrity. | Strong                 | High               | REST over HTTP         | REST offers a scalable, stateless API for managing profiles and relationships (e.g., GET /users/{id}), widely supported by social media clients and backend services. | Schema: users table with user_id (BIGINT PRIMARY KEY), username (VARCHAR), profile_data (JSONB); followers table for relationships. REST endpoints (e.g., GET /users/{id}/followers) manage social graphs, with PostgreSQL ensuring consistency for millions of users globally. | Logic: Implement social graph traversal (e.g., friends of friends). Data Structure: Use adjacency lists for relationships. Algorithm: Graph algorithms for friend suggestions. Interview Tip: Discuss large-scale social graphs and privacy controls. | Learn Facebook’s TAO system for social graph storage and PostgreSQL’s recursive CTEs for graph queries. Understand Twitter’s follower model (e.g., fanout limits) and Instagram’s privacy settings (e.g., private accounts). Explain LinkedIn’s connection degrees and Reddit’s karma system impact on schema design. | | |
| | |                                              | Feed Storage                      | Cassandra           | NoSQL (Wide-Column Store)         | High write throughput for massive-scale feed updates, time-series optimized for chronological data. | Tunable (Eventual)     | Massive            | Kafka                  | Kafka handles high-throughput, real-time feed update streams, ensuring reliable, ordered delivery of posts to followers across distributed systems. | Data Model: user_feeds table with user_id (Partition Key), timestamp (Clustering Key, DESC), columns: post_id, post_content. Kafka topics (e.g., feed_updates) distribute posts, enabling real-time feed generation for millions of users in platforms like Twitter or Instagram. | Logic: Fan-out on write vs. fan-out on read for feed generation. Data Structure: Wide rows for time-series storage. Algorithm: Feed ranking (e.g., edge rank). Interview Tip: Discuss personalization and pagination for infinite scrolling. | Master Twitter’s fan-out on write (e.g., 280-character tweets) and Instagram’s hybrid approach with precomputed feeds. Learn Cassandra’s TTL for post expiration (e.g., Reddit’s hot posts) and Kafka partitioning for feed scale. Explain Facebook’s EdgeRank and LinkedIn’s professional feed ranking nuances. | | |
| | |                                              | Media Storage                     | S3/GCS              | Object Storage                    | Scalable storage for images/videos, cost-effective for vast media, CDNs for low-latency global delivery. | Eventual               | Massive            | REST over HTTP         | REST provides a scalable, stateless API for uploading/retrieving media (e.g., PUT /media/{id}), leveraging HTTP’s binary data handling and CDN integration for global access. | Data Model: Key: media:{media_id}, Object: Media file. REST APIs (e.g., GET /media/{id}) fetch media from S3, served via CDNs like CloudFront, critical for social media where billions of images/videos are accessed daily. | Logic: Media transcoding for formats/resolutions. Data Structure: Metadata linking media to posts. Algorithm: Content moderation and duplicate detection. Interview Tip: Discuss mobile uploads and edge storage for faster access. | Understand Instagram’s image compression (e.g., WebP) and S3’s multi-part uploads for large videos (e.g., Facebook Reels). Learn Twitter’s media resizing and GCS’s versioning for edits. Explain CDN caching (e.g., Cloudflare) and Reddit’s media deduplication for cost efficiency at scale. | | |
| | |                                              | Caching Popular Content           | Redis               | NoSQL (In-memory Key-Value Store)  | Low-latency access to trending posts, reduces load on primary DBs, improves performance for high-demand content. | Eventual               | High               | Redis Pub/Sub          | Redis Pub/Sub broadcasts updates to popular content (e.g., viral posts) in real time, ensuring cache consistency across distributed nodes for fast delivery to users. | Data Model: Key: feed_cache:{feed_type}:{user_id}, Value: List of JSON posts. content_updates channel invalidates caches on new trends, optimizing performance where trending content drives engagement. | Logic: Scoring for trending content (e.g., engagement metrics). Data Structure: Redis Sorted Sets for ranking. Algorithm: Decaying function for time-based popularity. Interview Tip: Discuss invalidation for dynamic content and cache refresh mechanisms. | Learn Twitter’s trending algorithm (e.g., velocity of retweets) and Redis’ ZREVRANGE for top posts. Understand Instagram’s Explore page caching and Facebook’s News Feed pre-fetching. Explain Reddit’s hot score decay and LinkedIn’s professional content prioritization in caching strategies. | | |
| | |                                              | Full-Text Search                  | Elasticsearch       | Search Engine                     | Fast search for posts, profiles, hashtags, inverted index enables efficient and relevant results. | Eventual               | High               | REST over HTTP         | REST allows scalable, stateless querying of the search index (e.g., GET /search?q=hashtag), leveraging HTTP’s caching for quick, relevant search results across social content. | Data Model: posts_index, users_index with documents like PostDocument (content, hashtags). REST API (e.g., GET /search?q=#trending) queries Elasticsearch, delivering fast searches for millions of users in social media scenarios. | Logic: Autocomplete and suggestion features. Data Structure: Multi-field indexing for search types. Algorithm: Ranking by relevance and engagement. Interview Tip: Explain real-time indexing and sharding for scalability. | Master Twitter’s hashtag search (e.g., n-gram indexing) and Instagram’s Explore search (e.g., image tags). Learn Elasticsearch’s suggest API for autocomplete and Facebook’s unified search across posts/users. Discuss Reddit’s subreddit search and LinkedIn’s job search indexing strategies. | | |
| | | **Metrics Monitoring & Alerting (Google Analytics, Datadog)** | Time-Series Data Storage | InfluxDB            | Time-Series Database              | Efficient storage/querying of time-series metrics (CPU, memory, latency), optimized for high write/query loads. | Eventual               | High               | Kafka                  | Kafka processes high-throughput, real-time metric streams, ensuring reliable ingestion and ordered delivery to InfluxDB for analytics and monitoring. | Data Model: Measurements: cpu_usage, Tags: host, Fields: numeric values. Kafka topics (e.g., metrics) ingest data from distributed systems, enabling real-time dashboards in tools like Datadog, handling millions of metrics per second. | Logic: Downsampling for historical data. Data Structure: Tags for multidimensional querying. Algorithm: Anomaly detection algorithms. Interview Tip: Discuss high cardinality and efficient dashboard querying. | Understand InfluxDB’s retention policies (e.g., Google Analytics’ 30-day data) and Kafka’s partitioning for metric streams (e.g., Datadog’s 10M metrics/sec). Learn handling high cardinality (e.g., unique tags) and real-time aggregation for dashboards like GA’s traffic spikes or Datadog’s infrastructure monitoring. | | |
| | |                                              | Real-Time Alerting                | Redis               | NoSQL (In-memory Key-Value Store)  | Fast access to alert thresholds, real-time metric checks for anomalies, in-memory ensures quick evaluation. | Eventual               | Medium             | Redis Pub/Sub          | Redis Pub/Sub delivers real-time alert notifications to monitoring systems/users, ensuring timely responses to critical events with low latency. | Data Model: Key: alert_thresholds:{metric_name}, Value: threshold; Key: current_metric_value:{metric_name}, Value: current value. alerts channel broadcasts breaches, enabling instant notifications in monitoring systems like Google Analytics. | Logic: Hysteresis to prevent alert flapping. Data Structure: Redis Streams for event sourcing. Algorithm: Sliding window for threshold checks. Interview Tip: Explain handling alert storms and rate limiting notifications. | Learn Datadog’s alerting logic (e.g., multi-condition triggers) and Redis’ XADD for alert streams. Understand Google Analytics’ real-time event alerts and noise suppression (e.g., hysteresis thresholds). Discuss alert escalation (e.g., Slack integration) and scale handling like Datadog’s 100K alerts/day. | | |
| | | **Proximity Service (Tinder, Yelp, TripAdvisor)** | Geospatial Data Storage      | PostgreSQL with PostGIS | RDBMS (Geospatial Extension) | Advanced geospatial queries (proximity, polygons), robust spatial indexing (PostGIS) for mature geospatial DB. | Strong                 | Medium             | REST over HTTP         | REST provides a stateless API for querying geospatial data (e.g., GET /businesses/near?lat=lon), leveraging HTTP’s scalability and caching for location-based services. | Schema: businesses table with location (GEOGRAPHY(Point, 4326)). REST endpoints (e.g., /nearby?lat=40.7&lon=-74) use PostGIS ST_DWithin for proximity searches, critical for apps like Yelp finding nearby businesses accurately. | Logic: Efficient geospatial indexing (e.g., R-trees). Data Structure: Geospatial data types for accuracy. Algorithm: Haversine for distance. Interview Tip: Discuss large-scale geospatial data and caching frequent locations. | Master PostGIS ST_DWithin for Tinder’s swipe radius and Yelp’s business clustering. Learn PostgreSQL’s GiST indexes for geospatial scale and TripAdvisor’s polygon searches (e.g., city boundaries). Explain Tinder’s geosharding and Yelp’s review aggregation by location. | | |
| | |                                              | Real-Time Location Tracking       | RedisGeo            | NoSQL (In-memory Geospatial Store) | Ultra-low latency for real-time location updates, RedisGeo offers fast geospatial queries (GEORADIUS) for real-time data. | Eventual               | High               | Redis Pub/Sub          | Redis Pub/Sub broadcasts location updates to subscribed clients in real time, enabling instant proximity notifications for dynamic tracking scenarios. | Data Model: Index: user_locations, Members: user_id, Coordinates: Lat/Lon. location_updates channel pushes updates, with GEORADIUS queries matching users in real-time proximity services like Tinder, ensuring low-latency tracking for millions of users. | Logic: Geofencing for trigger actions. Data Structure: Redis Geo sets for proximity. Algorithm: Efficient geospatial indexing. Interview Tip: Discuss concurrent updates and WebSockets for notifications. | Understand Tinder’s GEORADIUS for matching (e.g., 10-mile radius) and Yelp’s real-time check-ins. Learn RedisGeo’s memory limits and TripAdvisor’s user movement tracking (e.g., hotel proximity). Discuss WebSocket scaling for Tinder’s 50M users and geofencing for promotions (e.g., Yelp deals). | | |
| | | **Google Maps, Waze**                       | Geospatial Data Storage           | PostgreSQL with PostGIS | RDBMS (Geospatial Extension) | Manages large-scale, complex geospatial datasets (roads, POIs), robust querying (PostGIS), production-grade. | Strong                 | Massive            | REST over HTTP         | REST enables efficient querying of geospatial data (e.g., GET /routes?start=end), leveraging HTTP’s caching and scalability for map services across global users. | Schema: Tables like roads (GEOGRAPHY linestrings), points_of_interest. REST APIs (e.g., /routes?start=lat1,lon1&end=lat2,lon2) use PostGIS for routing, serving billions of map requests daily in navigation systems. | Logic: Efficient routing (e.g., A*). Data Structure: Graph structures for road networks. Algorithm: Shortest path calculations. Interview Tip: Discuss traffic integration and caching popular routes. | Learn Google Maps’ road graph storage (e.g., linestrings) and PostGIS ST_ShortestPath for routing. Understand Waze’s POI updates and sharding geospatial data by region. Explain real-world scale (e.g., Google Maps’ 1B users) and traffic layer integration with historical data. | | |
| | |                                              | Real-Time Traffic Data            | Redis               | NoSQL (In-memory Key-Value Store)  | Low-latency access to dynamic traffic updates, caches data (speed, congestion) for real-time map rendering. | Eventual               | High               | Redis Pub/Sub          | Redis Pub/Sub distributes traffic updates instantly to map clients, ensuring up-to-date information with minimal latency for real-time navigation. | Data Model: Key: traffic_segment:{segment_id}, Value: { speed: numeric, congestion_level: string }. traffic_updates channel pushes sensor data, enabling real-time traffic overlays in apps like Waze, refreshing maps for millions of drivers continuously. | Logic: Traffic prediction models. Data Structure: Time-series for historical patterns. Algorithm: ML for forecasting. Interview Tip: Discuss high-frequency updates and edge computing for localized data. | Master Waze’s crowd-sourced traffic (e.g., Redis HSET for segments) and Google Maps’ ML-driven predictions. Learn handling 100K updates/sec and edge caching for urban areas. Explain real-time ETA adjustments and Waze’s incident reporting system integration with Redis Pub/Sub. | | |
| | | **Video Streaming (YouTube, Netflix, Twitch, TikTok Live)** | Video Metadata Storage    | PostgreSQL          | RDBMS                             | Strong consistency for metadata (title, uploader, tags), ACID compliance ensures reliable catalog storage. | Strong                 | High               | REST over HTTP         | REST provides a scalable, stateless API for managing video metadata (e.g., GET /videos/{id}), ensuring compatibility with clients for catalog browsing. | Schema: videos table with video_id (UUID PRIMARY KEY), title (VARCHAR), etc. REST endpoints (e.g., /videos/{id}) fetch metadata, with PostgreSQL supporting consistent catalog management for millions of videos in streaming platforms. | Logic: Efficient querying for recommendations. Data Structure: Indexes on tags/categories. Algorithm: Collaborative filtering for recommendations. Interview Tip: Discuss metadata updates and versioning for edits. | Learn YouTube’s tag system (e.g., GIN indexes) and Netflix’s genre categorization. Understand PostgreSQL’s partitioning for Twitch’s live streams and TikTok’s short video metadata. Explain real-world scale (e.g., YouTube’s 500 hours/minute upload) and metadata sync across regions. | | |
| | |                                              | Video Content Storage             | S3/GCS              | Object Storage                    | Scalable, durable storage for video files, cost-effective for large content, CDNs for global delivery. | Eventual               | Massive            | REST over HTTP         | REST enables efficient upload/retrieval of videos (e.g., GET /content/{video_id}), leveraging HTTP’s binary handling and CDN integration for low-latency streaming. | Data Model: Key: video_content:{video_id}, Object: MP4 file. REST APIs (e.g., GET /content/{id}) serve videos via CDNs, handling billions of streams daily in platforms like YouTube, with multiple resolutions stored as separate objects. | Logic: Adaptive bitrate streaming. Data Structure: Manifest files for playlists. Algorithm: Content-aware encoding. Interview Tip: Discuss mobile uploads and edge locations for delivery. | Master Netflix’s HLS/DASH streaming and S3’s multi-part uploads for YouTube uploads. Learn Twitch’s live transcoding and TikTok’s short video optimization (e.g., H.265). Explain CDN edge caching (e.g., Akamai) and real-world scale like Netflix’s 200M subscribers streaming globally. | | |
| | |                                              | User Session Management           | Redis               | NoSQL (In-memory Key-Value Store)  | Fast access to session data for real-time features (chat, history), in-memory for performance. | Eventual               | High               | Redis Pub/Sub          | Redis Pub/Sub synchronizes session updates (e.g., playback position) in real time across clients/services, ensuring seamless streaming experiences with low latency. | Data Model: Key: session:{session_id}, Value: { user_id: ObjectId, current_video_id: ObjectId, playback_position: numeric }. session_updates channel pushes changes, supporting real-time features like live chat or watch history in Netflix or Twitch. | Logic: Session expiration and renewal. Data Structure: Redis Hashes for session data. Algorithm: Session migration for load balancing. Interview Tip: Discuss consistency across devices and secure session management. | Understand Netflix’s multi-device sync (e.g., Redis HSET) and Twitch’s live chat sessions. Learn YouTube’s watch history sync and TikTok’s live session timeouts. Explain session hijacking prevention (e.g., JWT) and scale handling for Twitch’s 30M daily users during peak streams. | | |
| | |                                              | Recommendation Engine             | Cassandra           | NoSQL (Wide-Column Store)         | High write throughput for user interaction logs (views, likes), scales for massive activity data. | Eventual               | Massive            | Kafka                  | Kafka streams high-throughput interaction logs in real time, ensuring reliable ingestion for training recommendation models with ordered, fault-tolerant delivery. | Data Model: user_video_interactions table with user_id (Partition Key), timestamp (Clustering Key), columns: video_id, interaction_type. Kafka topics (e.g., user_interactions) feed Cassandra, powering real-time recommendations for millions of users on platforms like YouTube. | Logic: Real-time recommendation updates. Data Structure: Wide rows for interaction history. Algorithm: Matrix factorization for collaborative filtering. Interview Tip: Discuss cold start and hybrid recommendation systems. | Learn Netflix’s personalization (e.g., user-video matrix) and YouTube’s watch next (e.g., deep learning). Understand Cassandra’s COUNTER for interaction counts and Kafka’s role in TikTok’s For You page. Explain Twitch’s live recommendations and real-world cold start solutions (e.g., trending videos). | | |
| | | **Collaborative Document Editing (Google Docs, Notion)** | Document Storage          | Cassandra           | NoSQL (Wide-Column Store)         | High availability and scalability for real-time editing, ensures continuous access for concurrent edits. | Tunable (Eventual)     | Massive            | REST over HTTP         | REST provides a scalable, stateless API for managing document chunks (e.g., GET /documents/{id}), ensuring compatibility with clients for persistent storage operations. | Data Model: document_chunks table with document_id (Partition Key), chunk_index (Clustering Key), chunk_content (TEXT). REST endpoints (e.g., /documents/{id}/chunks) store chunks, with Cassandra scaling for collaborative edits by thousands of users simultaneously in tools like Google Docs. | Logic: OT or CRDTs for collaboration. Data Structure: Versioned document model. Algorithm: Diff and merge for conflict resolution. Interview Tip: Discuss concurrent edits and undo/redo functionality. | Master Google Docs’ OT implementation and Notion’s block-based storage (e.g., JSON chunks). Learn Cassandra’s tunable consistency for edits and sharding by doc ID. Explain real-world scale (e.g., Google Docs’ 1B users) and conflict resolution trade-offs (e.g., OT vs. CRDTs in Notion). | | |
| | |                                              | Real-Time Sync                    | Redis               | NoSQL (In-memory Key-Value Store)  | Ultra-low latency for real-time updates, Redis Pub/Sub for fast synchronization and conflict resolution. | Eventual               | High               | WebSocket              | WebSocket enables full-duplex, real-time communication for instant edit propagation, avoiding polling and ensuring low-latency sync across multiple users. | Data Model: Channels: document_updates:{document_id}. WebSocket pushes edit operations (e.g., via OT) to clients, ensuring seamless real-time collaboration in Google Docs-like systems, with Redis caching transient updates before persistence to Cassandra. | Logic: OT for conflict-free merging. Data Structure: Operational logs for edit history. Algorithm: Transformation functions for operations. Interview Tip: Discuss network partitions and OT vs. CRDT trade-offs. | Understand Google Docs’ OT specifics (e.g., operation ordering) and Notion’s WebSocket scaling (e.g., 100K concurrent edits). Learn Redis’ role in buffering edits and handling lag (e.g., throttling). Explain real-world sync failures (e.g., Google Docs’ “Trying to connect”) and Notion’s offline sync model. | | |
| | | **Trading Platforms (Robinhood, E*TRADE)** | Transaction Storage              | PostgreSQL          | RDBMS                             | Strong consistency and ACID compliance mandatory for financial transactions, ensures integrity for trades and settlements. | Strong                 | High               | REST over HTTP         | REST provides a secure, stateless API over HTTPS for managing transactions (e.g., POST /trades), ensuring reliable, auditable trade execution across clients and services. | Schema: transactions table with transaction_id (UUID PRIMARY KEY), account_id (INTEGER), trade_type (ENUM), etc. REST endpoints (e.g., /trades) ensure ACID-compliant trades, critical for trading platforms handling millions of transactions daily with regulatory compliance. | Logic: Two-phase commit for distributed transactions. Data Structure: Normalized tables for audit trails. Algorithm: Order matching engines. Interview Tip: Discuss high-frequency trading and fraud detection. | Learn Robinhood’s trade settlement (e.g., T+2) and PostgreSQL’s SERIALIZABLE isolation for trades. Understand E*TRADE’s order book storage and auditing transactions (e.g., FINRA compliance). Explain real-world scale (e.g., Robinhood’s 2021 outages) and fraud detection (e.g., velocity checks). | | |
| | |                                              | Real-Time Market Data             | Redis               | NoSQL (In-memory Key-Value Store)  | Low-latency access to live market feeds (quotes, order books), in-memory for rapid retrieval in high-frequency trading. | Eventual               | High               | Kafka                  | Kafka delivers high-throughput, real-time market data streams with order preservation and fault tolerance, essential for rapid dissemination to trading systems and users. | Data Model: Hashes: quote:{symbol} with { price: numeric, bid: numeric }; Sorted Sets for order books. Kafka topics (e.g., market_data) feed Redis, enabling sub-millisecond updates for stock prices in platforms like Robinhood, serving millions of traders instantly. | Logic: Efficient order book data structures. Data Structure: Redis Streams for market data. Algorithm: Low-latency dissemination. Interview Tip: Discuss data staleness and WebSockets for client updates. | Master Robinhood’s order book (e.g., ZADD for bids/asks) and Kafka’s role in E*TRADE’s market feeds (e.g., 1M quotes/sec). Learn handling data spikes (e.g., GameStop surge) and WebSocket scaling for real-time quotes. Explain latency optimization and real-world failures like Robinhood’s 2020 downtime. | | |
| | | **Ride-Sharing (Uber, Lyft, DoorDash, Uber Eats)** | Transaction Storage        | PostgreSQL          | RDBMS                             | Strong consistency for ride/payment transactions, ACID compliance ensures reliable billing and payouts. | Strong                 | High               | REST over HTTP         | REST offers a secure, stateless API over HTTPS for managing ride transactions (e.g., POST /rides), ensuring reliable integration with rider/driver apps and payment systems. | Schema: rides table with ride_id (UUID PRIMARY KEY), rider_id (INTEGER), fare (NUMERIC), etc. REST endpoints (e.g., /rides/{id}) ensure consistent billing, critical for ride-sharing apps processing millions of transactions daily with financial accuracy. | Logic: Fare calculation (distance, time, surge). Data Structure: Geospatial indexes for locations. Algorithm: Matching riders/drivers. Interview Tip: Discuss concurrent requests and cancellation policies. | Learn Uber’s surge pricing (e.g., dynamic multipliers) and PostgreSQL’s CHECK constraints for fare integrity. Understand Lyft’s payout system and DoorDash’s order linking. Explain Uber Eats’ transaction scale (e.g., 1B orders/year) and challenges like double-booking prevention. | | |
| | |                                              | Trip Logs                         | Cassandra           | NoSQL (Wide-Column Store)         | High write throughput for massive trip log volumes, efficient for analytics, auditing, and insights. | Eventual               | Massive            | Kafka                  | Kafka streams high-throughput trip logs in real time, ensuring reliable, ordered ingestion for analytics and operational monitoring across distributed systems. | Data Model: trip_logs table with driver_id (Partition Key), timestamp (Clustering Key), columns: ride_id, location. Kafka topics (e.g., trip_logs) feed Cassandra, supporting real-time analytics for millions of trips in apps like Uber, optimizing routing and driver performance. | Logic: Retention policies for compliance. Data Structure: Time-series partitioning. Algorithm: Anomaly detection for safety. Interview Tip: Discuss large-scale ingestion and stream processing for insights. | Understand Uber’s trip auditing (e.g., GDPR retention) and Cassandra’s TTL for logs. Learn Lyft’s route optimization and DoorDash’s delivery tracking via Kafka. Explain real-world analytics (e.g., Uber’s 15M trips/day) and anomaly detection for driver safety (e.g., sudden stops). | | |
| | |                                              | Real-Time Driver and Rider Matching | RedisGeo           | NoSQL (In-memory Geospatial Store) | Ultra-low latency for real-time location tracking, RedisGeo for fast geospatial queries (GEORADIUS) for matching. | Eventual               | High               | WebSocket              | WebSocket provides real-time, bi-directional communication for location updates and match notifications, ensuring instant driver-rider pairing without polling. | Data Model: Indexes: drivers_location, riders_location with driver_id, rider_id, and coordinates. WebSocket pushes updates (e.g., via ws://match) and uses GEORADIUS for matching, enabling real-time pairing in ride-sharing apps like Lyft for millions of users globally. | Logic: Matching with ETA and availability. Data Structure: Geospatial indexes for proximity. Algorithm: Load balancing across regions. Interview Tip: Discuss concurrency during peak hours and surge pricing. | Master Uber’s matching algorithm (e.g., GEORADIUS with ETA weighting) and Lyft’s driver availability checks. Learn DoorDash’s multi-party matching (e.g., restaurant, driver, customer) and WebSocket scaling for Uber Eats’ 10M daily active users. Explain surge pricing logic and peak handling (e.g., New Year’s Eve). | | |
| | |                                              | Geospatial Data Storage           | PostGIS             | RDBMS (Geospatial Extension)      | Advanced geospatial queries for routing, geocoding, PostGIS provides comprehensive tools for location services. | Strong                 | Medium             | REST over HTTP         | REST enables efficient querying of geospatial data (e.g., GET /routes?start=end), leveraging HTTP’s caching and scalability for route planning and location-based features. | Schema: road_segments table with route_geometry (GEOGRAPHY(Linestring, 4326)). REST APIs (e.g., /routes) use PostGIS for accurate routing, supporting navigation in ride-sharing apps with consistent geospatial data for millions of trips. | Logic: Routing for shortest paths. Data Structure: Graph DBs or spatial indexes. Algorithm: A* or Dijkstra’s for pathfinding. Interview Tip: Discuss traffic integration and caching popular routes. | Learn Uber’s routing with PostGIS ST_Dijkstra and Lyft’s geocoding for pickup points. Understand DoorDash’s delivery zone mapping and sharding road data. Explain real-world scale (e.g., Uber’s 7B trips/year) and traffic-aware routing (e.g., Waze integration). | | |
| | | **IoT & Smart Devices (Nest, Ring, Honeywell, Fitbit, Apple Watch)** | Sensor Data Storage | InfluxDB            | Time-Series Database              | Efficient storage/querying of time-series sensor data (temperature, motion), optimized for high-frequency readings. | Eventual               | Massive            | Kafka                  | Kafka scales to handle massive real-time sensor data streams from numerous devices, ensuring reliable, ordered ingestion for processing and analysis. | Data Model: sensor_readings measurement with Tags: device_id, Fields: sensor_value. Kafka topics (e.g., sensor_data) feed InfluxDB, managing billions of readings from IoT devices like Nest in smart home ecosystems. | Logic: Aggregation and downsampling. Data Structure: Tags for querying. Algorithm: Anomaly detection for maintenance. Interview Tip: Discuss high-frequency ingestion and real-time dashboards. | Understand Nest’s thermostat data (e.g., temp every 10s) and InfluxDB’s continuous queries for aggregation. Learn Ring’s motion logs and Fitbit’s fitness tracking scale (e.g., 100M devices). Explain Kafka’s role in Apple Watch’s heart rate streaming and real-world dashboards (e.g., Nest’s energy reports). | | |
| | |                                              | User Profile Storage              | PostgreSQL          | RDBMS                             | Strong consistency for profiles, device configs, settings, ACID compliance ensures integrity for user control. | Strong                 | Medium             | REST over HTTP         | REST provides a scalable, stateless API for managing profiles/settings (e.g., GET /users/{id}), ensuring compatibility with IoT apps and dashboards for reliable access. | Schema: iot_users table with user_id (SERIAL PRIMARY KEY), user_profile (JSONB); iot_devices table links devices. REST endpoints (e.g., /users/{id}/devices) ensure consistent management for IoT ecosystems like Fitbit with millions of users. | Logic: Device authentication/authorization. Data Structure: Foreign keys for relationships. Algorithm: Secure key management. Interview Tip: Discuss registration and OAuth for authorization. | Learn Nest’s device pairing (e.g., OAuth flows) and PostgreSQL’s UNIQUE constraints for user-device links. Understand Ring’s user permissions and Fitbit’s profile sync (e.g., 500M workouts). Explain Apple Watch’s secure enclave integration and real-world privacy (e.g., GDPR for IoT data). | | |
| | |                                              | Real-Time Alerts                  | Redis               | NoSQL (In-memory Key-Value Store)  | Fast access to thresholds, real-time sensor checks for anomalies, in-memory for quick alert evaluation. | Eventual               | High               | Redis Pub/Sub          | Redis Pub/Sub broadcasts alerts instantly to monitoring systems/users, ensuring timely responses to critical events like temperature spikes with low latency. | Data Model: Key: iot_alert_thresholds:{device_id}:{sensor_type}, Value: threshold; Key: current_sensor_value:{device_id}:{sensor_type}, Value: current value. alerts channel notifies apps like Ring of anomalies, enabling real-time responses in IoT networks. | Logic: Alert suppression for noise. Data Structure: Redis Streams for sourcing. Algorithm: Complex event processing. Interview Tip: Discuss false positives and customizable rules. | Master Ring’s motion alerts (e.g., Redis XADD) and Nest’s temp alerts (e.g., hysteresis). Learn Fitbit’s heart rate anomaly detection and Apple Watch’s fall detection logic. Explain real-world scale (e.g., Ring’s 10M alerts/day) and alert customization (e.g., user-defined thresholds). | | |
| | | **Search Engines (Google Search, Bing)**   | Index Storage                     | Elasticsearch       | Search Engine                     | Fast full-text search over massive web-scale datasets, inverted index for speed, sharding/replication for scale. | Eventual               | Massive            | REST over HTTP         | REST provides a scalable, stateless API for querying the index (e.g., GET /search?q=query), leveraging HTTP’s caching and simplicity for billions of searches. | Data Model: Sharded indexes with optimized inverted structures. REST API (e.g., /search?q=news) queries Elasticsearch, handling billions of searches daily in engines like Google with rapid, relevant results. | Logic: Distributed inverted index. Data Structure: Postings lists for mapping. Algorithm: TF-IDF or BM25 for ranking. Interview Tip: Discuss real-time indexing and personalized results. | Learn Google’s PageRank integration with Elasticsearch-like indexes and Bing’s keyword boosting. Understand sharding strategies (e.g., by term or doc) and handling 100B searches/day. Explain real-time crawling (e.g., Google News) and personalization (e.g., user history in Bing). | | |
| | |                                              | Metadata Storage                  | PostgreSQL          | RDBMS                             | Strong consistency for search metadata (page ranks, crawl status), ACID compliance for reliable indexing pipeline. | Strong                 | Massive            | REST over HTTP         | REST manages metadata reliably (e.g., GET /metadata/{url}), ensuring compatibility with crawlers and indexing services for consistent metadata access. | Schema: Tables like url_metadata (page rank, crawl time). REST endpoints (e.g., /metadata/{url}) support massive-scale metadata management in search engines, with PostgreSQL ensuring consistency for billions of URLs crawled daily. | Logic: Crawling schedulers. Data Structure: Priority queues for scheduling. Algorithm: PageRank for relevance. Interview Tip: Discuss duplicate content and canonical URLs. | Master Google’s crawl budget (e.g., PostgreSQL PRIORITY) and Bing’s sitemap storage. Learn deduplicating URLs (e.g., hash-based) and scaling metadata for 50B pages. Explain real-world challenges like Google’s robots.txt handling and Bing’s freshness updates (e.g., news indexing). | | |
| | | **Payment Systems (PayPal, Stripe, Venmo, Cash App)** | Transaction Storage     | PostgreSQL          | RDBMS                             | Strong consistency and ACID compliance essential for financial transactions, ensures integrity for processing/settlement. | Strong                 | Massive            | REST over HTTP         | REST over HTTPS provides a secure, stateless API for transactions (e.g., POST /payments), widely supported by payment gateways and clients for reliable, auditable operations. | Schema: payments table with payment_id (UUID PRIMARY KEY), amount (NUMERIC), etc. REST endpoints (e.g., /payments) ensure PCI-compliant transactions, critical for payment systems processing billions of dollars daily with security and auditing. | Logic: Idempotency for requests. Data Structure: Normalized tables for audits. Algorithm: Fraud detection with ML. Interview Tip: Discuss currency conversion and multi-currency support. | Learn PayPal’s idempotency keys and PostgreSQL’s UNIQUE constraints for payments. Understand Stripe’s chargeback handling and Venmo’s social payments (e.g., comments). Explain Cash App’s Bitcoin integration and real-world scale (e.g., PayPal’s 22B transactions/year). | | |
| | |                                              | Fraud Detection Logs              | Cassandra           | NoSQL (Wide-Column Store)         | High write throughput for logging all transaction data for fraud analysis, handles extreme log volumes for real-time/batch analysis. | Eventual               | Massive            | Kafka                  | Kafka ensures high-throughput, real-time ingestion of transaction logs, maintaining order and reliability for fraud detection systems analyzing millions of events. | Data Model: fraud_logs table with transaction_date (Partition Key), timestamp (Clustering Key), columns: transaction_id, transaction_details (JSON). Kafka topics (e.g., fraud_logs) feed Cassandra, enabling real-time fraud analysis in platforms like Stripe with massive transaction volumes. | Logic: Real-time fraud scoring. Data Structure: Wide rows for history. Algorithm: Anomaly detection. Interview Tip: Discuss false positives and ML for prevention. | Understand Stripe’s Radar (e.g., ML scoring) and Cassandra’s partitioning for logs. Learn PayPal’s velocity checks and Venmo’s social fraud detection (e.g., duplicate payments). Explain Kafka’s role in Cash App’s fraud logs and real-world scale (e.g., 10M transactions/day). | | |
| | | **Object Storage (AWS S3, Google Cloud Storage)** | Metadata Storage            | Cassandra           | NoSQL (Wide-Column Store)         | Scalable, fault-tolerant storage for object metadata (filenames, sizes), high availability for metadata access. | Tunable (Eventual)     | Massive            | REST over HTTP         | REST provides a scalable, stateless API for managing metadata (e.g., GET /objects/{key}/metadata), ensuring compatibility with clients and services for object operations. | Data Model: object_metadata table with bucket_id (Partition Key), object_key (Clustering Key), columns: size (BIGINT), etc. REST APIs (e.g., /buckets/{id}/objects) manage metadata, with Cassandra scaling for billions of objects in storage systems like S3. | Logic: Eventual consistency for updates. Data Structure: Distributed hash tables. Algorithm: Consistent hashing. Interview Tip: Discuss versioning and ACLs. | Learn S3’s metadata consistency model (e.g., eventual writes) and Cassandra’s role in GCS versioning. Understand bucket sharding and handling 100M objects/bucket. Explain real-world use (e.g., S3’s 100T objects) and ACLs for secure access (e.g., IAM policies). | | |
| | |                                              | File Storage                      | Ceph/HDFS           | Distributed File System           | Durable, scalable storage for objects, ensures redundancy (replication, erasure coding) and availability. | Strong                 | Massive            | REST over HTTP         | REST enables efficient upload/retrieval of files (e.g., PUT /objects/{key}), leveraging HTTP’s binary handling and scalability for massive object storage access globally. | Data Model: Files split into replicated/erasure-coded blocks. REST APIs (e.g., GET /objects/{key}) access files, with Ceph/HDFS ensuring durability for petabytes of data in cloud storage like AWS S3, integrated with CDNs for global delivery. | Logic: Data replication strategies. Data Structure: Block storage with metadata. Algorithm: Erasure coding. Interview Tip: Discuss large uploads and multipart uploads. | Master Ceph’s CRUSH algorithm for S3-like storage and HDFS’s namenode for GCS. Learn erasure coding trade-offs (e.g., 4+2 vs. replication) and handling 10PB scale. Explain S3’s multipart upload for large files (e.g., 5TB) and real-world durability (e.g., 11 9s). | | |
| | | **Real-Time Gaming (Fortnite, PUBG)**       | State Management                  | Redis               | NoSQL (In-memory Key-Value Store)  | Ultra-low latency for real-time game state updates (positions, actions), in-memory essential for responsive gameplay. | Eventual               | High               | WebSocket              | WebSocket supports real-time, bi-directional communication for game state updates, ensuring low-latency player interactions without polling in fast-paced gaming environments. | Data Model: Hashes: player:{player_id} with { x_pos: numeric, health: numeric }. WebSocket (e.g., ws://game/state) pushes updates, with Redis enabling sub-millisecond state sync for millions of players in multiplayer games like Fortnite. | Logic: State sync (e.g., delta updates). Data Structure: Spatial partitioning. Algorithm: Lag compensation. Interview Tip: Discuss latency and anti-cheat measures. | Learn Fortnite’s player sync (e.g., 100-player matches) and Redis’ HSET for state. Understand PUBG’s spatial partitioning (e.g., quadtrees) and WebSocket scaling for 1M concurrent users. Explain lag compensation (e.g., server rewind) and real-world anti-cheat (e.g., Fortnite’s BattlEye). | | |
| | |                                              | Persistent Player Data            | PostgreSQL          | RDBMS                             | Strong consistency for profiles, progress, achievements, ACID compliance for reliable player data and economies. | Strong                 | Medium             | REST over HTTP         | REST provides a secure, stateless API for managing player data (e.g., GET /players/{id}), ensuring compatibility with game clients and services for persistent storage ops. | Schema: players table with player_id (SERIAL PRIMARY KEY), player_profile (JSONB), etc. REST endpoints (e.g., /players/{id}) ensure consistent progress tracking, critical for gaming platforms with millions of users maintaining inventories and achievements. | Logic: Inventory management. Data Structure: Normalized tables for items. Algorithm: Leaderboard ranking. Interview Tip: Discuss purchases and currency transactions. | Understand Fortnite’s item shop (e.g., ACID transactions) and PostgreSQL’s PARTITION BY for player data. Learn PUBG’s stats tracking and scaling for 50M players. Explain real-world economies (e.g., V-Bucks) and leaderboard sharding (e.g., top 1000 players). | | |
| | | **E-Commerce (Amazon, eBay)**               | Product Search Indexing           | Elasticsearch       | Search Engine                     | Fast search for product catalog, inverted index optimized for queries (keyword, filtering, faceting). | Eventual               | Massive            | REST over HTTP         | REST delivers scalable, stateless search queries (e.g., GET /products/search?q=phone), leveraging HTTP’s caching for rapid, relevant results in e-commerce browsing. | Data Model: products_index with ProductDocument (name, description, category). REST API (e.g., /search?q=laptop) queries Elasticsearch, supporting millions of searches daily in platforms like Amazon with faceted navigation. | Logic: Faceted search for filtering. Data Structure: Multi-field indexing. Algorithm: Personalized ranking. Interview Tip: Discuss relevance and autocomplete. | Master Amazon’s A9 search (e.g., faceting by price) and Elasticsearch’s aggs for filters. Learn eBay’s keyword boosting and handling 1B searches/day. Explain autocomplete (e.g., Amazon’s typeahead) and real-world personalization (e.g., buy-again suggestions). | | |
| | |                                              | Transaction Storage               | PostgreSQL          | RDBMS                             | Strong consistency for orders/payments, ACID compliance ensures reliable processing, payment handling, inventory updates. | Strong                 | Massive            | REST over HTTP         | REST over HTTPS provides a secure, stateless API for transactions (e.g., POST /orders), ensuring reliable, auditable order processing across clients and services. | Schema: orders table with order_id (UUID PRIMARY KEY), order_items (JSONB ARRAY); payments table links payments. REST endpoints (e.g., /orders) ensure ACID-compliant transactions, critical for e-commerce handling billions of orders with inventory sync. | Logic: Inventory reservation. Data Structure: Locking for updates. Algorithm: Distributed transactions. Interview Tip: Discuss fulfillment and saga patterns. | Learn Amazon’s 1-Click checkout (e.g., SERIALIZABLE isolation) and PostgreSQL’s row-level locking. Understand eBay’s auction settlements and handling 300M orders/day. Explain real-world failures (e.g., Amazon’s 2018 Prime Day crash) and saga orchestration (e.g., order rollback). | | |
| | |                                              | Product Catalog                   | DynamoDB            | NoSQL (Key-Value Store)           | Flexible schema and scalability for diverse product data (attributes vary by category), handles large datasets efficiently. | Eventual               | Massive            | REST over HTTP         | REST offers a scalable, stateless API for catalog management (e.g., GET /products/{id}), integrating seamlessly with e-commerce apps and services for flexible product access. | Data Model: product_catalog table with product_id (String, Primary Key), variable attributes. REST APIs (e.g., /products/{id}) fetch products, with DynamoDB scaling for millions of diverse items in e-commerce like eBay. | Logic: Schema evolution for attributes. Data Structure: Single-table design. Algorithm: Secondary indexes for querying. Interview Tip: Discuss variations and caching strategies. | Understand Amazon’s catalog scale (e.g., 500M SKUs) and DynamoDB’s single-table design (e.g., category#id). Learn eBay’s attribute flexibility (e.g., used vs. new) and querying efficiently. Explain real-world catalog sync (e.g., Amazon’s third-party sellers) and caching (e.g., DynamoDB DAX). | | |
| | |                                              | Caching Popular Products          | Redis               | NoSQL (In-memory Key-Value Store)  | Low-latency access to frequently viewed products, reduces catalog DB load during peak traffic, improves UX. | Eventual               | High               | Redis Pub/Sub          | Redis Pub/Sub notifies services of product updates (e.g., price changes) in real time, ensuring cache consistency across distributed systems for fast delivery during sales. | Data Model: Key: product_cache:{product_id}, Value: JSON product details. product_updates channel invalidates caches, optimizing performance during high-traffic events like Black Friday on Amazon. | Logic: Pre-warming for demand. Data Structure: Redis Hashes for products. Algorithm: Invalidation on updates. Interview Tip: Discuss stampedes and distributed locks. | Learn Amazon’s Prime Day caching (e.g., Redis HMSET) and eBay’s auction countdowns. Understand Redis Cluster for scale (e.g., 100M products) and handling Black Friday spikes (e.g., 1B requests/hour). Explain stampede prevention (e.g., jittered TTL) and real-world price update sync. | | |
| | |                                              | Recommendation Engine             | Neo4j               | Graph Database                    | Efficient relationship modeling for personalized recommendations, graph traversal for user-product interactions. | Eventual               | Medium             | REST over HTTP         | REST provides a stateless API for querying recommendations (e.g., GET /recommendations/{user_id}), leveraging HTTP’s simplicity for integrating with e-commerce frontends. | Data Model: Nodes: User, Product; Relationships: VIEWED, PURCHASED. REST API (e.g., /recommendations/{id}) uses Cypher queries to traverse graphs, delivering tailored suggestions for millions of users on Amazon based on purchase history. | Logic: Graph-based recommendations. Data Structure: Property graphs. Algorithm: Traversal for similarity. Interview Tip: Discuss real-time and hybrid models. | Master Amazon’s “Customers who bought” (e.g., Cypher traversals) and eBay’s bid-based recs. Learn Neo4j’s sharding for 1B users and real-time updates (e.g., Kafka integration). Explain hybrid recs (e.g., content + collaborative) and real-world scale (e.g., Amazon’s 50% sales from recs). | | |
| | | **Weather Services (National Weather Service, AccuWeather)** | Time-Series Data Storage | InfluxDB            | Time-Series Database              | Efficient storage/querying of weather metrics (temperature, humidity), optimized for time-based analytics. | Eventual               | High               | Kafka                  | Kafka handles high-throughput, real-time weather data streams from stations/sensors, ensuring reliable ingestion for forecasting and reporting with ordered delivery. | Data Model: weather_metrics measurement with Tags: location_id, Fields: metric_value. Kafka topics (e.g., weather_data) feed InfluxDB, supporting real-time weather updates for millions of users in apps like AccuWeather. | Logic: Interpolation for missing values. Data Structure: Continuous queries. Algorithm: Forecasting with historical data. Interview Tip: Discuss high-frequency data and ML predictions. | Learn AccuWeather’s minute-by-minute data (e.g., InfluxDB tags) and NWS’s station feeds via Kafka. Understand interpolation (e.g., linear for temp) and handling 1M updates/hour. Explain real-world forecasting (e.g., AccuWeather’s 1.5B daily requests) and ML models (e.g., storm prediction). | | |
| | |                                              | Geospatial Data Storage           | PostGIS             | RDBMS (Geospatial Extension)      | Advanced geospatial queries for location-based forecasts, PostGIS for robust mapping and spatial analysis. | Strong                 | Medium             | REST over HTTP         | REST enables querying geospatial weather data (e.g., GET /forecast?lat=lon), leveraging HTTP’s caching and scalability for location-specific forecasts. | Schema: weather_stations with location (GEOGRAPHY(Point, 4326)); forecast_grids with grid_geometry (GEOGRAPHY(Polygon, 4326)). REST APIs (e.g., /forecast?lat=40.7&lon=-74) use PostGIS for accurate forecasts, serving weather apps with precise regional data. | Logic: Spatial joins for aggregation. Data Structure: Raster data for grids. Algorithm: Interpolation for predictions. Interview Tip: Discuss large-scale queries and caching locations. | Master NWS’s grid forecasts (e.g., PostGIS ST_Intersects) and AccuWeather’s radar maps. Learn raster storage for precipitation and scaling for 10K stations. Explain real-world geospatial scale (e.g., 1B daily forecasts) and caching for urban areas (e.g., NYC weather). | | |
| | | **Customer Support (Zendesk, LiveChat)**    | Ticket Storage                    | PostgreSQL          | RDBMS                             | Strong consistency for ticket data, updates, history, ACID compliance for issue tracking and resolution. | Strong                 | Medium             | REST over HTTP         | REST provides a scalable, stateless API for managing tickets (e.g., POST /tickets), ensuring compatibility with support clients and dashboards for reliable operations. | Schema: support_tickets with ticket_id (UUID PRIMARY KEY), status (ENUM); ticket_comments links comments. REST endpoints (e.g., /tickets/{id}) ensure consistent tracking for thousands of customer interactions in platforms like Zendesk. | Logic: Assignment/escalation workflows. Data Structure: Foreign keys for links. Algorithm: Priority queuing. Interview Tip: Discuss merging and AI responses. | Learn Zendesk’s ticket states (e.g., PENDING) and PostgreSQL’s TRIGGER for updates. Understand LiveChat’s real-time ticket creation and handling 1M tickets/month. Explain real-world automation (e.g., Zendesk’s macros) and ticket merging (e.g., duplicate detection). | | |
| | |                                              | Full-Text Search                  | Elasticsearch       | Search Engine                     | Fast search for tickets, knowledge base, notes, enhances agent efficiency in finding solutions. | Eventual               | Medium             | REST over HTTP         | REST delivers scalable search queries (e.g., GET /search?q=issue), leveraging HTTP’s simplicity and caching for quick access to support content by agents. | Data Model: tickets_index, knowledge_base_index with documents like TicketDocument (subject, description). REST API (e.g., /search?q=error) queries Elasticsearch, speeding up issue resolution in customer support systems with thousands of tickets daily. | Logic: Multi-field search. Data Structure: Nested docs for comments. Algorithm: Ranking by status/recency. Interview Tip: Discuss relevance and ML query understanding. | Master Zendesk’s search across tickets/articles (e.g., multi_match) and LiveChat’s chat history indexing. Learn scaling for 100K daily searches and real-world relevance (e.g., prioritizing open tickets). Explain ML-driven suggestions (e.g., Zendesk’s Answer Bot) and knowledge base search optimization. | | |
| | | **Travel Booking (Expedia, Booking.com)**   | Reservation Storage               | PostgreSQL          | RDBMS                             | Strong consistency and integrity for bookings, ACID compliance prevents overbooking and ensures accuracy. | Strong                 | High               | REST over HTTP         | REST over HTTPS provides a secure, stateless API for bookings (e.g., POST /reservations), ensuring reliable integration with travel apps and services to prevent double-booking. | Schema: reservations table with reservation_id (UUID PRIMARY KEY), check_in_date (DATE), etc. REST endpoints (e.g., /reservations) ensure ACID-compliant bookings, critical for travel platforms managing millions of reservations with precise availability. | Logic: Inventory to prevent overbooking. Data Structure: Composite keys for lookups. Algorithm: Confirmation workflows. Interview Tip: Discuss concurrent bookings and cancellations. | Learn Booking.com’s room inventory (e.g., UNIQUE constraints) and Expedia’s flight bookings (e.g., seat maps). Understand PostgreSQL’s FOR UPDATE for locks and handling 10M bookings/day. Explain real-world overbooking prevention (e.g., Expedia’s 2019 glitch) and cancellation flows. | | |
| | |                                              | Availability Cache                | Redis               | NoSQL (In-memory Key-Value Store)  | Low-latency access to real-time availability (flights, hotels), reduces load on reservation DB during peak traffic. | Eventual               | High               | Redis Pub/Sub          | Redis Pub/Sub notifies services of availability changes (e.g., bookings) in real time, ensuring cache consistency across distributed systems for accurate, fast checks. | Data Model: Key: availability:{property_id}:{date}, Value: inventory count. availability_updates channel invalidates caches, optimizing real-time checks in travel apps like Expedia during high-demand periods like holidays. | Logic: Invalidation on bookings. Data Structure: Redis Hashes for availability. Algorithm: Optimistic locking for updates. Interview Tip: Discuss consistency and distributed locks. | Understand Expedia’s flight seat caching (e.g., HINCRBY) and Booking.com’s hotel availability sync. Learn Redis’ role in holiday peaks (e.g., 1M queries/sec) and handling real-world scale (e.g., Booking.com’s 1.5M properties). Explain lock-free updates and cache coherence across regions. | | |
| | | **Music Streaming (Spotify, Pandora)**      | Metadata Storage                  | PostgreSQL          | RDBMS                             | Strong consistency for song, artist, album metadata, ACID compliance for structured library data integrity. | Strong                 | High               | REST over HTTP         | REST provides a scalable, stateless API for managing music metadata (e.g., GET /songs/{id}), ensuring compatibility with clients for library browsing and playlist creation. | Schema: songs table with song_id (UUID PRIMARY KEY), title (VARCHAR); related tables: artists, albums. REST endpoints (e.g., /songs/{id}) ensure consistent metadata for millions of tracks in streaming services like Spotify. | Logic: Querying for playlists. Data Structure: Indexes on artist/genre. Algorithm: Fingerprinting for identification. Interview Tip: Discuss updates and caching popular songs. | Learn Spotify’s catalog (e.g., 70M songs) and PostgreSQL’s JOIN for artist lookups. Understand Pandora’s genre tagging and scaling for 100M users. Explain real-world metadata sync (e.g., Spotify’s daily updates) and audio fingerprinting (e.g., Shazam-like ID matching). | | |
| | |                                              | Audio Storage                     | S3/GCS              | Object Storage                    | Scalable, durable storage for audio files, cost-effective for massive libraries, CDNs for low-latency streaming. | Eventual               | Massive            | REST over HTTP         | REST enables efficient upload/retrieval of audio (e.g., GET /audio/{song_id}), leveraging HTTP’s binary handling and CDN integration for global, low-latency streaming. | Data Model: Key: audio:{song_id}, Object: MP3/FLAC file. REST APIs (e.g., /audio/{id}) serve audio via CDNs, supporting billions of streams daily in platforms like Pandora with multiple bitrate variants stored as separate objects. | Logic: Transcoding for formats. Data Structure: Manifests for streaming. Algorithm: Content-aware encoding. Interview Tip: Discuss uploads and edge caching. | Master Spotify’s AAC streaming (e.g., S3 manifests) and Pandora’s multi-bitrate storage. Learn GCS’s role in 1B daily streams and handling uploads (e.g., artist submissions). Explain real-world CDN use (e.g., Spotify’s 600M users) and edge caching for low latency. | | |
| | |                                              | Recommendation Engine             | Neo4j               | Graph Database                    | Efficient relationship modeling for personalized playlists, graph traversal for user music taste analysis. | Eventual               | Medium             | REST over HTTP         | REST provides a stateless API for querying recommendations (e.g., GET /recommendations/{user_id}), integrating easily with streaming apps for personalized playlist delivery. | Data Model: Nodes: User, Song, Artist; Relationships: LISTENED_TO, LIKES. REST API (e.g., /recommendations/{id}) uses Cypher queries to generate playlists, powering music recommendations for millions of Spotify users based on listening habits. | Logic: Collaborative filtering with graphs. Data Structure: Property graphs. Algorithm: Community detection for genres. Interview Tip: Discuss cold start and real-time updates. | Learn Spotify’s Discover Weekly (e.g., Neo4j traversals) and Pandora’s Music Genome Project (e.g., genre clustering). Understand scaling for 400M users and real-time updates (e.g., Kafka). Explain cold start (e.g., new users) and real-world impact (e.g., Spotify’s 30% stream boost from recs). | | |
| | | **Coding Platforms (LeetCode, CodePen)**    | Submission Storage                | PostgreSQL          | RDBMS                             | Strong consistency for submissions, results, evaluations, ACID compliance for data integrity. | Strong                 | Medium             | REST over HTTP         | REST provides a scalable, stateless API for managing submissions (e.g., POST /submissions), ensuring reliable integration with coding clients and evaluation systems. | Schema: code_submissions table with submission_id (UUID PRIMARY KEY), submission_code (TEXT), test_case_results (JSONB ARRAY). REST endpoints (e.g., /submissions) ensure consistent storage for thousands of submissions daily in platforms like LeetCode. | Logic: Sandboxes for security. Data Structure: Normalized tables. Algorithm: Plagiarism detection. Interview Tip: Discuss resource-intensive executions and complexity analysis. | Understand LeetCode’s execution sandbox (e.g., Docker) and PostgreSQL’s JSONB for results. Learn CodePen’s live previews and scaling for 1M submissions/day. Explain real-world plagiarism (e.g., LeetCode’s similarity checks) and time/space complexity tracking (e.g., O(n) runtime). | | |
| | |                                              | Full-Text Search                  | Elasticsearch       | Search Engine | |
| | | System Design / Application                  | Service                           | Database            | Database Type                      | Why Chosen                                                                                              | Data Consistency Needs | Scale Requirements | Communication Protocol | Why Chosen for Protocol                                                                                              | Additional Details                                                                                                                           | Key Insights for System Design Interviews                                                                                           | Specific Knowledge to Learn for Interviews                                                                                                    | | |
| | |----------------------------------------------|-----------------------------------|---------------------|------------------------------------|---------------------------------------------------------------------------------------------------------|------------------------|--------------------|------------------------|----------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------| | |
| | | **Coding Platforms (LeetCode, CodePen)**    | Full-Text Search                  | Elasticsearch       | Search Engine                     | Fast search for problems, solutions, snippets, enhances learning with quick content discovery.            | Eventual               | Medium             | REST over HTTP         | REST delivers scalable search queries (e.g., GET /search?q=algorithm), leveraging HTTP’s simplicity and caching for rapid access to coding content by users. | Data Model: problems_index, solutions_index with documents like ProblemDocument (title, description). REST API (e.g., /search?q=sort) queries Elasticsearch, supporting coding platforms with fast problem/solution searches for thousands of learners. | Logic: Code-specific analyzers. Data Structure: Inverted indexes for snippets. Algorithm: Ranking by difficulty/ratings. Interview Tip: Discuss syntax highlighting and ML recommendations. | Learn LeetCode’s problem search (e.g., tag filtering) and Elasticsearch’s match for code keywords. Understand CodePen’s snippet search and scaling for 10M queries/month. Explain real-world relevance (e.g., difficulty-based ranking) and ML-driven problem suggestions (e.g., LeetCode Premium). | | |
| | | **Voice Assistants (Alexa, Siri)**          | Knowledge Graph                   | Neo4j               | Graph Database                    | Complex relationship modeling for knowledge representation, graph DBs excel at interconnected entities for natural language understanding. | Eventual               | Medium             | REST over HTTP         | REST provides a stateless API for querying the knowledge graph (e.g., GET /knowledge?query=weather), ensuring compatibility with voice assistant clients for contextual answers. | Data Model: Nodes: Entities (e.g., places); Relationships: LOCATED_IN. REST API (e.g., /knowledge?query=Paris) uses Cypher queries, enabling voice assistants like Alexa to answer complex questions for millions of users based on interconnected data. | Logic: Entity resolution/disambiguation. Data Structure: Property graphs. Algorithm: Graph traversal for answers. Interview Tip: Discuss updates and NLU. | Master Alexa’s skill graph (e.g., MATCH queries) and Siri’s entity links (e.g., people to places). Learn scaling for 100M daily queries and real-time updates (e.g., news). Explain real-world disambiguation (e.g., “Paris” city vs. person) and NLU integration (e.g., intent parsing). | | |
| | |                                              | User Data Storage                 | PostgreSQL          | RDBMS                             | Strong consistency for preferences, profiles, history, ACID compliance for personalized experiences.     | Strong                 | Medium             | REST over HTTP         | REST manages user data reliably (e.g., GET /users/{id}), ensuring compatibility with assistant apps and services for consistent personalization across interactions. | Schema: voice_assistant_users with user_id (SERIAL PRIMARY KEY), user_preferences (JSONB); conversation_history links logs. REST endpoints (e.g., /users/{id}) ensure consistent profiles for assistants managing millions of user interactions. | Logic: Authentication/authorization. Data Structure: Normalized tables. Algorithm: Personalization via history. Interview Tip: Discuss privacy and ML intent prediction. | Learn Alexa’s user profiles (e.g., voice ID) and PostgreSQL’s PARTITION BY for history. Understand Siri’s iCloud sync and handling 500M users. Explain real-world privacy (e.g., Alexa’s data deletion) and intent prediction (e.g., Siri’s proactive suggestions). | | |
| | |                                              | Log Storage                       | MongoDB             | NoSQL (Document Store)            | Flexible schema for diverse log data (transcripts, intents), schema-less for varied interaction logs.    | Eventual               | Medium             | REST over HTTP         | REST provides a straightforward, stateless API for storing/retrieving logs (e.g., POST /logs), integrating with analytics tools for monitoring assistant performance. | Data Model: voice_assistant_logs collection with { _id: ObjectId, timestamp: ISODate(), interaction_type: "command" }. REST endpoints (e.g., /logs) store logs, with MongoDB scaling for thousands of daily interactions in assistants like Siri. | Logic: Aggregation for analytics. Data Structure: Embedded docs for interactions. Algorithm: Sentiment analysis. Interview Tip: Discuss ingestion and real-time monitoring. | Understand Alexa’s skill analytics (e.g., MongoDB aggregate) and Siri’s command logs. Learn scaling for 1B daily interactions and real-time monitoring (e.g., error rates). Explain real-world sentiment (e.g., user frustration) and log pruning (e.g., GDPR compliance). | | |
| | | **Auction Platforms (Sotheby’s Online)**   | Real-Time Bidding                 | Redis               | NoSQL (In-memory Key-Value Store)  | Ultra-low latency for bid updates, in-memory ensures fast processing and responsiveness for live auctions. | Eventual               | High               | WebSocket              | WebSocket enables real-time, bi-directional bid updates, pushing current bids to all participants instantly, ensuring a dynamic, competitive experience without polling. | Data Model: Hashes: auction:{auction_id}:item:{item_id} with { current_bid: numeric }; Sorted Sets for bid ranking. WebSocket (e.g., ws://auction/bids) pushes updates, critical for real-time bidding in online auctions with thousands of participants globally. | Logic: Bid validation/anti-sniping. Data Structure: Sorted Sets for ordering. Algorithm: Auction closing with extensions. Interview Tip: Discuss concurrent bids and fairness. | Learn Sotheby’s live bidding (e.g., Redis ZINCRBY) and handling 10K bids/sec. Understand anti-sniping (e.g., bid extensions) and WebSocket scaling for 100K viewers. Explain real-world fairness (e.g., bid timestamping) and auction scale (e.g., Sotheby’s $5B annual sales). | | |
| | |                                              | Transaction Storage               | PostgreSQL          | RDBMS                             | Strong consistency for results, payments, settlements, ACID compliance essential for auditing and finalization. | Strong                 | High               | REST over HTTP         | REST over HTTPS provides a secure, stateless API for managing transactions (e.g., POST /auctions), ensuring reliable, auditable settlement across clients and services. | Schema: auctions with auction_id (UUID PRIMARY KEY), winning_bid_id (UUID); bids links bids. REST endpoints (e.g., /auctions/{id}) ensure ACID-compliant settlements, vital for auction platforms handling high-value transactions with legal compliance. | Logic: Payment processing/escrow. Data Structure: Normalized tables. Algorithm: Winner determination. Interview Tip: Discuss disputes and blockchain transparency. | Understand Sotheby’s escrow system (e.g., PostgreSQL TRANSACTION) and bid history tracking. Learn scaling for 1M bids/day and handling disputes (e.g., bid retractions). Explain real-world settlement (e.g., Sotheby’s multi-currency) and blockchain potential (e.g., NFT auctions). | | |
| | | **Video Conferencing (Zoom, Microsoft Teams)** | Scheduling Storage            | PostgreSQL          | RDBMS                             | Strong consistency for schedules, invites, participant lists, ACID compliance for reliable meeting setup. | Strong                 | Medium             | REST over HTTP         | REST provides a scalable, stateless API for managing schedules (e.g., POST /meetings), ensuring compatibility with calendar integrations and clients for consistent planning. | Schema: meetings table with meeting_id (UUID PRIMARY KEY), start_time (TIMESTAMP), invitees (INTEGER ARRAY). REST endpoints (e.g., /meetings) ensure consistent scheduling for thousands of meetings daily in conferencing tools like Zoom. | Logic: Calendar sync/conflict detection. Data Structure: Recurrence rules. Algorithm: Availability querying. Interview Tip: Discuss time zones and reminders. | Learn Zoom’s meeting IDs (e.g., unique constraints) and Teams’ Outlook sync (e.g., iCal rules). Understand PostgreSQL’s INTERVAL for recurrence and handling 50M daily meetings. Explain real-world time zone handling (e.g., Zoom’s global users) and reminder systems (e.g., Teams notifications). | | |
| | |                                              | Session Management                | Redis               | NoSQL (In-memory Key-Value Store)  | Fast access to session data (participants, status, chat), in-memory for live features during conferences. | Eventual               | High               | WebSocket              | WebSocket supports real-time, low-latency communication for session updates (e.g., joins, chat), ensuring seamless interactions without polling in video calls. | Data Model: Key: video_conference_session:{meeting_id}, Value: { participants: [user_id], chat_messages: [...] }. WebSocket (e.g., ws://session/{id}) pushes updates, enabling real-time features like chat and speaker tracking for millions of users in platforms like Teams. | Logic: Session sync across participants. Data Structure: Hashes for metadata. Algorithm: Conflict resolution. Interview Tip: Discuss partitions and WebRTC. | Master Zoom’s participant sync (e.g., Redis HSET) and Teams’ chat persistence. Learn WebSocket scaling for 300M daily Zoom users and WebRTC for P2P streaming. Explain real-world failures (e.g., Zoom’s 2020 outages) and session migration (e.g., load balancing across regions). | | |
| | | **Review Platforms (Yelp, TripAdvisor)**    | Review Storage                    | PostgreSQL          | RDBMS                             | Strong consistency for reviews, ratings, business info, ACID compliance for data integrity.              | Strong                 | Medium             | REST over HTTP         | REST provides a scalable, stateless API for managing reviews (e.g., POST /reviews), ensuring compatibility with clients and services for reliable submission and retrieval. | Schema: reviews with review_id (UUID PRIMARY KEY), rating (NUMERIC); businesses with location (GEOGRAPHY(Point, 4326)). REST endpoints (e.g., /reviews) ensure consistent storage for thousands of daily submissions in platforms like Yelp. | Logic: Moderation/flagging. Data Structure: Foreign keys for links. Algorithm: Sentiment analysis. Interview Tip: Discuss spam and trust scores. | Learn Yelp’s review moderation (e.g., TRIGGER for flags) and TripAdvisor’s rating system. Understand PostgreSQL’s CHECK for valid ratings and handling 10M reviews/month. Explain real-world spam detection (e.g., Yelp’s filters) and sentiment analysis (e.g., NLP for TripAdvisor). | | |
| | |                                              | Full-Text Search                  | Elasticsearch       | Search Engine                     | Fast search for reviews, businesses, locations, enhances UX with quick discovery of relevant content.    | Eventual               | Medium             | REST over HTTP         | REST delivers scalable search queries (e.g., GET /search?q=restaurant), leveraging HTTP’s caching for rapid access to review content by users. | Data Model: reviews_index, businesses_index with documents like ReviewDocument (text, rating). REST API (e.g., /search?q=cafe) queries Elasticsearch, supporting review platforms with fast, location-aware searches for millions of users. | Logic: Geospatial search. Data Structure: Geospatial fields. Algorithm: Ranking by recency/rating. Interview Tip: Discuss personalization and ML queries. | Master Yelp’s “near me” search (e.g., geo_point) and TripAdvisor’s hotel ranking. Learn scaling for 1B searches/year and real-world personalization (e.g., user prefs). Explain ML-driven relevance (e.g., Yelp’s Elite reviews) and geospatial boosts (e.g., proximity weighting). | | |
| | |                                              | Geospatial Data Storage           | PostGIS             | RDBMS (Geospatial Extension)      | Advanced geospatial queries for location-based searches (nearby businesses), PostGIS for robust geospatial functions. | Strong                 | Medium             | REST over HTTP         | REST enables querying geospatial data (e.g., GET /businesses/near?lat=lon), leveraging HTTP’s scalability and caching for location-based browsing. | Schema: Reuses businesses table with location (GEOGRAPHY(Point, 4326)). REST APIs (e.g., /nearby?lat=40.7&lon=-74) use PostGIS ST_DWithin, ensuring accurate searches in review platforms like TripAdvisor serving millions of travelers. | Logic: Geospatial indexing. Data Structure: Geospatial types. Algorithm: Distance-based sorting. Interview Tip: Discuss large-scale data and caching. | Understand Yelp’s business proximity (e.g., ST_Distance) and TripAdvisor’s destination maps. Learn PostGIS scaling for 50M businesses and caching hotspot areas (e.g., NYC). Explain real-world geospatial queries (e.g., Yelp’s 200M reviews) and sorting by distance/rating. | | |
| | | **Online Learning (Coursera, Udemy)**       | Course Storage                    | PostgreSQL          | RDBMS                             | Strong consistency for course metadata, enrollments, ACID compliance for structured data integrity.     | Strong                 | Medium             | REST over HTTP         | REST provides a scalable, stateless API for managing courses (e.g., GET /courses/{id}), ensuring compatibility with learning platforms for consistent delivery. | Schema: courses with course_id (UUID PRIMARY KEY), modules (JSONB ARRAY); enrollments links users. REST endpoints (e.g., /courses/{id}) ensure consistent management for thousands of courses in platforms like Coursera. | Logic: Progress tracking/certification. Data Structure: Normalized tables. Algorithm: Course recommendations. Interview Tip: Discuss versioning and prerequisites. | Learn Coursera’s module structure (e.g., JSONB nesting) and Udemy’s enrollment tracking. Understand PostgreSQL’s CHECK for progress and handling 10M learners. Explain real-world certification (e.g., Coursera’s blockchain certs) and prerequisite enforcement (e.g., course sequences). | | |
| | |                                              | User Log Storage                  | MongoDB             | NoSQL (Document Store)            | Flexible schema for diverse activity logs (course access, video views), adaptable to varied log types.   | Eventual               | Medium             | REST over HTTP         | REST offers a straightforward API for storing/retrieving logs (e.g., POST /logs), integrating with analytics tools for monitoring learner progress and engagement. | Data Model: online_learning_logs collection with { _id: ObjectId, event_type: "video_view", course_id: ObjectId }. REST endpoints (e.g., /logs) store logs, with MongoDB scaling for thousands of daily learner interactions in online learning systems. | Logic: Behavior analytics. Data Structure: Embedded docs. Algorithm: Clustering for segmentation. Interview Tip: Discuss ingestion and dropout prediction. | Understand Coursera’s progress logs (e.g., aggregate for analytics) and Udemy’s video views. Learn scaling for 50M users and real-time analytics (e.g., completion rates). Explain real-world dropout prediction (e.g., ML models) and log retention (e.g., 90-day policies). | | |
| | |                                              | Video Storage                     | S3/GCS              | Object Storage                    | Scalable, durable storage for course videos, efficient for content, CDNs for low-latency streaming.      | Eventual               | Massive            | REST over HTTP         | REST enables efficient upload/retrieval of videos (e.g., GET /videos/{video_id}), leveraging HTTP’s binary handling and CDN integration for global streaming. | Data Model: Key: course_video:{video_id}, Object: MP4 file. REST APIs (e.g., /videos/{id}) serve videos via CDNs, supporting millions of learners streaming content in platforms like Udemy with high availability. | Logic: Transcoding for devices. Data Structure: Manifests for streaming. Algorithm: Content-aware encoding. Interview Tip: Discuss uploads and edge caching. | Learn Coursera’s lecture streaming (e.g., HLS manifests) and Udemy’s instructor uploads (e.g., S3 multi-part). Understand GCS’s role in 1B video views/month and CDN optimization (e.g., Cloudflare). Explain real-world scale (e.g., Coursera’s 100M learners) and transcoding pipelines (e.g., 1080p vs. 4K). | | |
| | | **Advertising Platforms (Google Ads, Facebook Ads)** | Analytics Storage          | ClickHouse          | Columnar Database                 | Fast aggregations and queries for ad metrics (impressions, clicks), columnar storage optimized for analytics. | Eventual               | Massive            | Kafka                  | Kafka processes high-throughput, real-time ad event streams (e.g., clicks), ensuring reliable ingestion for analytics with ordered, fault-tolerant delivery to ClickHouse. | Data Model: ad_performance_daily table with date (Date), impressions (UInt64). Kafka topics (e.g., ad_events) feed ClickHouse, enabling real-time performance tracking for billions of impressions in platforms like Google Ads. | Logic: Real-time bidding analytics. Data Structure: Columnar for aggregation. Algorithm: CTR prediction. Interview Tip: Discuss ingestion and attribution models. | Understand Google Ads’ impression tracking (e.g., ClickHouse SUM) and Facebook Ads’ click analytics. Learn Kafka’s role in 10B events/day and scaling for real-time reporting. Explain real-world attribution (e.g., last-click vs. multi-touch) and CTR prediction (e.g., ML models). | | |
| | |                                              | Real-Time Bidding                 | Redis               | NoSQL (In-memory Key-Value Store)  | Ultra-low latency for real-time ad auctions, in-memory ensures fast bid processing in high-frequency bidding. | Eventual               | High               | Kafka                  | Kafka handles high-frequency bid requests/responses in real time, ensuring reliable, ordered processing critical for auction-based platforms with massive scale. | Data Model: Hashes: ad_auction:{auction_id} with { current_bid: numeric }; Sorted Sets for bid ranking. Kafka topics (e.g., bids) feed Redis, supporting sub-millisecond bidding for millions of auctions daily in ad exchanges like Google Ad Exchange. | Logic: Auction mechanisms (second-price). Data Structure: Priority queues. Algorithm: Decisioning for ad selection. Interview Tip: Discuss timeouts and ML optimization. | Master Google Ads’ RTB (e.g., Redis ZADD for bids) and Facebook’s auction dynamics (e.g., second-price). Learn Kafka scaling for 1M auctions/sec and handling timeouts (e.g., 100ms). Explain real-world bid optimization (e.g., ML for CTR) and scale (e.g., Google Ads’ $200B revenue). | | |
| | | **Sports News (ESPN, Yahoo Sports)**        | Metadata Storage                  | PostgreSQL          | RDBMS                             | Strong consistency for articles, player data, ACID compliance for structured sports data integrity.      | Strong                 | Medium             | REST over HTTP         | REST provides a scalable, stateless API for managing sports metadata (e.g., GET /articles/{id}), ensuring compatibility with news apps and services for reliable access. | Schema: articles with article_id (UUID PRIMARY KEY), sport_category_id (INTEGER); players with player_stats (JSONB). REST endpoints (e.g., /articles/{id}) ensure consistent metadata for thousands of articles daily in platforms like ESPN. | Logic: Tagging/categorization. Data Structure: Indexes on categories. Algorithm: Related article recs. Interview Tip: Discuss real-time updates and caching. | Learn ESPN’s player stats (e.g., JSONB parsing) and Yahoo Sports’ article tagging. Understand PostgreSQL’s GIN indexes for categories and handling 1M articles/month. Explain real-world updates (e.g., ESPN’s live scores) and caching (e.g., breaking news). | | |
| | |                                              | Real-Time Updates                 | Cassandra           | NoSQL (Wide-Column Store)         | High write throughput for live scores, events, stats, handles large volumes of real-time sports updates.  | Eventual               | Massive            | Kafka                  | Kafka distributes high-throughput sports updates (e.g., scores) in real time, ensuring reliable, ordered delivery to Cassandra for live event streaming to users. | Data Model: live_scores table with game_id (Partition Key), timestamp (Clustering Key), event_details (JSON). Kafka topics (e.g., sports_updates) feed Cassandra, enabling real-time score updates for millions of fans in apps like Yahoo Sports. | Logic: Event sourcing for state. Data Structure: Time-series partitioning. Algorithm: Consistency checks. Interview Tip: Discuss high-frequency and WebSockets. | Understand ESPN’s live scores (e.g., Cassandra TTL) and Yahoo Sports’ game events via Kafka. Learn scaling for 100K updates/min and real-time consistency (e.g., score disputes). Explain WebSocket integration (e.g., ESPN’s 50M users) and real-world event sourcing (e.g., play-by-play logs). | | |
| | | **Cloudflare CDN**                          | Configuration Storage             | PostgreSQL          | RDBMS                             | Strong consistency for CDN configs, ACID compliance for critical settings/rules, reliable and auditable management. | Strong                 | High               | REST over HTTP         | REST over HTTPS provides a secure, stateless API for managing configs (e.g., POST /configs), ensuring compatibility with admin tools and services for consistent setup. | Schema: Tables like cdn_rules (routing, caching rules), security_settings (firewall, SSL). REST endpoints (e.g., /configs) ensure consistent rule management for millions of domains in CDNs like Cloudflare with strict versioning and auditing. | Logic: Versioning/rollback. Data Structure: Normalized history. Algorithm: Rule precedence. Interview Tip: Discuss propagation and blue-green updates. | Learn Cloudflare’s DNS rules (e.g., PostgreSQL versioning) and handling millions of domains. Understand propagation delays and blue-green deployments. Explain real-world scale (e.g., Cloudflare’s 20M+ domains) and security settings (e.g., SSL management). | | |
